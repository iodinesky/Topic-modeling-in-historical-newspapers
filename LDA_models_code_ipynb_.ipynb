{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z28m9R8SQKdt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.chdir('/content/LDA_data')\n",
        "data = os.listdir()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8My67rRcAdg"
      },
      "outputs": [],
      "source": [
        "! python -m spacy download ru_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bo92B4XVrlJh"
      },
      "outputs": [],
      "source": [
        "import nltk as nltk\n",
        "import re\n",
        "from string import punctuation\n",
        "import json\n",
        "import glob\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "stopwords = ['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n",
        "\n",
        "\n",
        "def lemmatization(texts, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
        "    # nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
        "    nlp = spacy.load(\"ru_core_news_sm\", disable=[\"parser\", \"ner\"])\n",
        "    texts_out = []\n",
        "    for text in texts:\n",
        "        doc = nlp(text)\n",
        "        new_text = []\n",
        "        for token in doc:\n",
        "            if token.pos_ in allowed_postags:\n",
        "                new_text.append(token.lemma_)\n",
        "        final = \" \".join(new_text)\n",
        "        texts_out.append(final)\n",
        "    return texts_out\n",
        "\n",
        "\n",
        "\n",
        "def gen_words(texts):\n",
        "    final = []\n",
        "    for text in texts:\n",
        "        new = gensim.utils.simple_preprocess(text, deacc=True)\n",
        "        new_1 = [token for token in new if token not in stopwords]\n",
        "        final.append(new_1)\n",
        "    return (final)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4XpZjqoFtBM"
      },
      "outputs": [],
      "source": [
        "search_list = ['облигация', 'процент', 'проц', 'займ', 'заем', 'фонд', 'бумага', 'дисконт', \n",
        "               'купон', 'рента', 'ипотечныи', 'ценности', 'доходность', 'котировка', 'эмиссия',\n",
        "               'баланс', 'дивиденд', 'прибыль', 'акционерныи', 'капитал', 'акция', 'акционер', 'собрание', 'правление']\n",
        "search_list"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pyLDAvis"
      ],
      "metadata": {
        "id": "jUSl8izCU9wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTqORiwwguyY"
      },
      "outputs": [],
      "source": [
        "from gensim.test.utils import datapath\n",
        "import re\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "\n",
        "issues_with_page = []\n",
        "lda_models = []\n",
        "lda_topics = []\n",
        "\n",
        "counter = 0\n",
        "for text in data:\n",
        "    print(text)\n",
        "    print(counter)\n",
        "    os.chdir('/content/LDA_data')\n",
        "    f = open(text, encoding='Windows-1251', mode='r')\n",
        "    issue_text = f.read()\n",
        "    f.close()\n",
        "    split_regex = re.compile(r'[.|!|?|…]')\n",
        "    sentences = filter(lambda t: t, [t.strip() for t in split_regex.split(issue_text)])\n",
        "    data_all = []\n",
        "    for s in sentences:\n",
        "       data_all.append(s)\n",
        "\n",
        "    lemmatized_texts = lemmatization(data_all)\n",
        "    data_words = gen_words(lemmatized_texts)\n",
        "    new_data_text = list(data_words)\n",
        "    id2word = corpora.Dictionary(new_data_text)\n",
        "    texts = new_data_text\n",
        "    corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                            id2word=id2word,\n",
        "                                            num_topics=5,\n",
        "                                            random_state=100,\n",
        "                                            update_every=1,\n",
        "                                            chunksize=50,\n",
        "                                            passes=10,\n",
        "                                            alpha=\"auto\")\n",
        "    \n",
        "    list_of_topics = lda_model.print_topics()\n",
        "    for topic in list_of_topics: \n",
        "        string = topic[1]\n",
        "        for word in search_list:\n",
        "            index = string.find(word)\n",
        "            word_weight = 0\n",
        "            if index != -1:\n",
        "                try: \n",
        "                    word_weight = float(string[index-7:index-2])\n",
        "                except:\n",
        "                        continue\n",
        "            if word_weight > 0.010:\n",
        "                issues_with_page.append(text[:-4])\n",
        "                lda_topics.append(lda_model.print_topics())\n",
        "    \n",
        "\n",
        "    # os.chdir('/content/LDA_models')\n",
        "    # vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word, mds=\"mmds\", R=10)\n",
        "    # pyLDAvis.save_html(vis, f\"{text[:-4]}\" + '_lda_model.html')\n",
        "\n",
        "    counter += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "    \n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iowL3ve7A80"
      },
      "outputs": [],
      "source": [
        "issues_with_page_dict = issues_with_page\n",
        "lda_topics_dict = lda_topics\n",
        "dict_to_pd = dict(zip(issues_with_page_dict, lda_topics_dict))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyzAeahC7wum"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(dict_to_pd)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install openpyxl"
      ],
      "metadata": {
        "id": "q8koBwVDHDAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Сохранение отобранных LDA моделей в виде таблицы Excel\n",
        "\n",
        "os.chdir('/content/Excel_df')\n",
        "df = df.T\n",
        "writer = pd.ExcelWriter('output.xlsx')\n",
        "df.to_excel(writer)\n",
        "writer.save()"
      ],
      "metadata": {
        "id": "79tlsMpJH3u3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/LDAmodels.zip /content/Folder_To_Zip"
      ],
      "metadata": {
        "id": "DIWqQLXCOxUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# сохранение визуализаций моделей в архив\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "fantasy_zip = zipfile.ZipFile('/content/LDA_zip/archive.zip', 'w')\n",
        "for folder, subfolders, files in os.walk('/content/LDA_models'):\n",
        "    for file in files:\n",
        "            fantasy_zip.write(os.path.join(folder, file), os.path.relpath(os.path.join(folder,file), 'C:\\\\Stories\\\\Fantasy'), compress_type = zipfile.ZIP_DEFLATED)\n",
        "fantasy_zip.close()"
      ],
      "metadata": {
        "id": "lfJNv4z3O4nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9y7Kwx0gnGn"
      },
      "outputs": [],
      "source": [
        "# Compute Perplexity\n",
        "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))\n",
        "\n",
        "# Compute Coherence Score\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=new_data_text, dictionary=id2word, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Удалить папку LDA_data\n",
        "\n",
        "import shutil \n",
        " \n",
        "dir_path = '/content/LDA_data/'\n",
        "\n",
        "try:\n",
        "    shutil.rmtree(dir_path)\n",
        "except OSError as e:\n",
        "    print(\"Error: %s : %s\" % (dir_path, e.strerror))"
      ],
      "metadata": {
        "id": "JPxoFUleOTFt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}